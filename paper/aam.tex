Abstract machines are a versatile, clear and concise way to describe the semantics of programming languages.
%
They hit a sweet spot in terms of level of discourse, and viable implementation strategy.
%
First year graduate students learn programming language semantics with abstract machines~\cite{dvanhorn:Felleisen2009Semantics}.
%\footnote{Northeastern's CS7400, UMD's CMSC631, University of Utah's CS7520}
%
They also turn out to be fairly simple to repurpose into static analyses for the languages they implement, via the Abstracting Abstract Machines methodology~\citep{dvanhorn:VanHorn2010Abstracting}.
%
The basic idea is that abstract machines implement a language's \emph{concrete} semantics, so we transform them slightly so that they also implement a language's \emph{abstract} semantics (thus ``abstracting'' abstract machines).

AAM is founded on three ideas:
\begin{enumerate}
\item{concrete and abstract semantics ideally should use the same code, for correctness and testing purposes,}
\item{the level of approximation should be a tunable parameter,}
\item{both of the above are achievable with a slight change to the abstract machine's state representation.}
\end{enumerate}

The first two points are the philosophy of AAM: correctness through simplicity, reusability, and sanity checking with concrete semantics.
%
The final point is the machinery that we recount in this section.

% \subsection{Notations and conventions}
% \begin{itemize}
% \item{$D \finto R$ \textbf{:} a finite function from domain $D$ to codomain $R$.}
% \item{$\setbuild{\mathit{term}}{\mathit{formula}}$ \textbf{:} a set comprehension where the free variables are implicitly existentially bound. Consider it a set denotation of a $\Sigma$ type.}
% \item{We use $\widehat{\mathit{space}}$ to say a space is an abstracted form of the concrete semantics' analogous $\mathit{space}$. We will not add hats to spaces that are added to the abstraction.}
% \item{Similarly, elements of abstract spaces have hats: \eg{} $\makont \in \sa{Kont}$.}
% \item{We will write $n$-tuples as $\tpl{t_1,\ldots,t_n}$, $(t_1,\ldots,t_n)$ or just $t_1,\ldots,t_n$, and their $i^\text{th}$ projections as $\pi_i$.}
% \item{$\sqcup$ \textbf{:} a join operator that lifts pointwise over tuples and functions. Sets are unioned. Ground terms are implicitly lifted to singleton sets.}
% \item{$S^*$ \textbf{:} the space of lists of elements of $S$.}
% \item{$\epsilon$ \textbf{:} the empty list.}
% \item{$\kcons{a}{l}$ \textbf{:} a list $l$ with $a$ added to the front.}
% \item{$\append{l}{l'}$ \textbf{:} list $l'$ appended to list $l$.}
% \item{$g \to t, e$ \textbf{:} Dijkstra notation for ``if $g$ then $t$ else $e$''}
% \item{$t \deceq t'$ \textbf{:} a Boolean decision for equality of terms $t$ and $t'$.}
% \item{$\extm{\menv}{\mvar}{\maddr}$ \textbf{:} $\lambda \mvaralt. \mvar \deceq \mvaralt \to \maddr, \menv(\mvaralt)$.}
% \item{$\mathit{elem} \in \mathit{Space} ::= \mathit{term} \alt \ldots$ \textbf{:} a set of Backus-Nauer-form alternates that inductively define a space $\mathit{Space}$. The $\mathit{term}$s in each alternate are constructors where element variables like $\mathit{elem}$ denote their containing space. Unwrapped elements of other spaces are implicitly injected. Refined constructors of other spaces are overloaded for the refining space.

% For example:
% \begin{align*}
%   \mvar \in \mathit{Variable} &\text{ a set} \\
%   \mexpr \in \Expr &::= \svar{\mvar} \alt \sapp{\mexpr}{\mexpr} \alt \slam{\mvar}{\mexpr} \\
%   \mval \in \Value &::= \slam{\mvar}{\mexpr}
% \end{align*}
% can be written as inductive datatypes with no overloading
% \begin{verbatim}
% Parameter Variable : Set.
% Inductive Expr : Set :=
% | var : Variable -> Expr
% | app : Expr -> Expr -> Expr
% | lam : Variable -> Expr -> Expr.
% Inductive Value : Set :=
% | vlam : Variable -> Expr -> Value.
% \end{verbatim}}
% \item{We use element variables to treat tuples with record syntax. For example, For a space defined like $\mstate \in \State ::= \tpl{\mexpr,\menv,\mkont}$, we may write $\mstate.\mkont$ to mean that there exists some $\mexpr',\menv',\mkont'$ such that $\mstate = \tpl{\mexpr',\menv',\mkont'}$ and $\mstate.\mkont = \mkont'$. We may also write $\mstate\{\menv := \bot\}$ to mean $\tpl{\mexpr',\bot,\mkont'}$.}
% \item{$s \stepto s'$ \textbf{:} a state $s$ steps to another state $s'$ via a reduction relation $\stepto \subseteq \State \times \State$. The relation should be clear from the context and the space of $s, s'$.}
% \item{$\mtrace$ \textbf{:} a trace with respect to some reduction relation $\stepto$, \ie{}, a sequence of states $\set{s_i}_{i=0}^n$ such that this proposition holds $\IsTrace(\mtrace) = \forall i \in [0..n-1). s_i \stepto s_{i+1}$}
% \item{We implicitly reassociate tuples or splice them into a larger context to have a cleaner presentation.
% For example, we may write $\tpl{\menv(\mexpr), \mkont}$ for an element of $\Expr \times \Env \times \Kont$ instead of ``$\tpl{\mval,\menv',\mkont}$ where $(\mval,\menv') = \menv(\mvar)$.''}
% \end{itemize}

\subsection{The $\boldsymbol\CESKstart$ machine \emph{schema}}

The case studies in this paper have full implementations in PLT Redex~\citep{dvanhorn:Felleisen2009Semantics} available online.\footnote{\url{http://github.com/dvanhorn/aac}}
They all build off the untyped call-by-value lambda calculus, whose semantics we recall in small-step reduction semantics style.


\paragraph{From $\boldsymbol{\beta_v}$ to CEK - Being explicit:}

Expressions consist of variables, applications, and
$\lambda$-abstractions, which are considered values:
\[
\begin{array}{lclcl}
\mexpr & \in & \mathit{Expr} & ::= & \svar\mvar \alt \sapp{\mexpr}{\mexpr} \alt \mval\\[1mm]
\mval  & \in & \Value        & ::= & \slam{\mvar}{\mexpr}
\end{array}
\]
Reduction is characterized by a binary relation, $\beta_v$, which
reduces the application of a function to a value by substituting the
argument for the formal parameter of the function:
\[
\begin{array}{rcl}
(\slam{\mvar}{\mexpr})\;\mval & \beta_v & [\mval/\mvar]\mexpr
\end{array}
\]

A search strategy is given for specifying the order in which
function applications are selected for reduction.  This strategy is
specified as a grammar of evaluation contexts:
\[
\begin{array}{lclcl}
E & \in & \mathit{EvaluationContext} & ::= & [] \alt \sapp{E}{\mexpr} \alt \sapp{\mval}{E} 
\end{array}
\]
This grammar specifies that applications are reduced left-to-right,
and reduction does not occur under $\lambda$-abstractions.  The ``[]''
context is the empty ``hole'' context.  The notation $E[\mexpr]$ means
the expression obtained by replacing the hole of $E$ with
$\mexpr$.

\newcommand{\betastep}{\mathrel{\longmapsto_{\beta_v}}}
Finally, reduction on closed expressions is then given by the $\betastep$ relation:
\begin{align*}
\mexpr \betastep \mexpr' \text{ if } \mexpr = E[\mexpr_0], \mexpr' = E[\mexpr_1]\text{, and }\mexpr_0\;\beta_v\;\mexpr_1\text,\\
\text{
for some $E$, $\mexpr_0$, and $\mexpr_1$.}
\end{align*}


Although concise and well-structured for mathematical proofs, this
semantics sheds little light on the mechanics of an effective
implementation. There are essentially two aspects that need to be
modeled more concretely.

The first aspect is that of substitution.  The $\beta_v$ axiom models
function application via the meta-theoretic notion of substition, but
substitution can be modeled more explicitly by incorporating
an \emph{environment}, which maps a variable to the value that should
be substituted for it.  This gives rise to the notion of
a \emph{closure}, which represents values by $\lambda$-abstractions
paired together with an environment.  The $\beta_v$-axiom is replaced
by an alternative axiom that interprets function application by
extending the environment of a closure with the formal parameter
mapping to the argument in the scope of the body of the function;
it interprets variables, by looking up their value in the environment.

The second aspect is that of search.  The $\betastep$ relation is
defined in terms of evaluation contexts, but gives no guidance on how
to construct this context.  The search for reduction opportunities can
be modeled more explicitly by a stack that represents the evaluation
context in an ``inside-out'' fashion: the inner-most part of the
context is the top frame of the stack.

Modeling substution with closures and evaluation contexts with stacks
yields an abstract machine for the language, the CEK
machine~\cite{dvanhorn:Felleisen2009Semantics}, shown in \autoref{fig:cek}.

\begin{figure}\centering
  \begin{tabular}{rlrl}
    $\mstate \in \CEK$ &\hspace{-3mm}$::= \tpl{\mexpr, \menv, \mkont}$
    & $\mlam \in \Lam$ &\hspace{-3mm}$::= \slam{\mvar}{\mexpr}$\\
    $\mval \in \Value$ &\hspace{-3mm}$::= (\mlam,\menv)$ 
    & $\menv \in \Env$ &\hspace{-3mm}$= \Var \finto \Value$ \\
    $\mkont \in \Kont$ &\hspace{-3mm}$= \Frame^*$ &
    $\mvar \in \Var$ &\\ %\hspace{-3mm} a set\\
    $\mkframe \in \Frame$ &\multicolumn{2}{l}{\hspace{-3mm}$::= \appl{\mexpr,\menv} \alt \appr{\mval}$} \\[2mm]
  \end{tabular}

  $\mstate \stepto \mstate'$ \\[.5mm]
  \begin{tabular}{r|l}
    \hline\vspace{-3mm}\\
    $\tpl{\svar\mvar, \menv, \mkont}$
    &
    $\tpl{\mval, \mkont}$ if $\mval = \menv(\mvar)$
    \\
% Application
    $\tpl{\sapp{\mexpri0}{\mexpri1},\menv,\mkont}$
    &
    $\tpl{\mexpri0,\menv,\kcons{\appl{\mexpri1,\menv}}{\mkont}}$
    \\
% Arg eval
    $\tpl{\mval, \kcons{\appl{\mexpr,\menv}}{\mkont}}$
    &
    $\tpl{\mexpr,\menv,\kcons{\appr{\mval}}{\mkont}}$
    \\
% Function call
    $\tpl{\mval,\kcons{\appr{\slam{\mvar}{\mexpr},\menv}}{\mkont}}$
    &
    $\tpl{\mexpr,\extm{\menv}{\mvar}{\mval},\mkont}$
  \end{tabular}
  \caption{CEK machine}
  \label{fig:cek}
\end{figure}


\paragraph{From CEK to CESK$_t^\star$ - Accounting for space \& time:}

The CEK machine makes explicit the mechanics of substitution and
reduction in context, but doesn't give an explicit account of memory
allocation.  We now reformulate the CEK machine into a heap-based
machine that explicitly allocates memory for variable bindings and
stack frames.  Properly accounting for memory allocation will prove
essential for making computable abstractions, as the key mechanism for
soundly approximating the running of a program will be limiting the
memory available to analyze a program.

The memory heap is modeled as a \emph{store} mapping addresses to
values or frames.  Environments are modified to map variables to
addresses and when a function is applied, we allocate space in the
heap to hold the value of the argument.
%
To account for the space used by the stack, we change the
representation of the continuation from a inductively defined list of
frames to a linked list structure that is explicitly allocated in the
heap.

In anticipation of approximating machines, we make a slight
departure from the standard model and have the store map addresses to
\emph{sets of} values or frames, denoting the possible values an address may take on.


The CESK$_t^\star$ machine is defined in \autoref{fig:ceskstart}.
%
% The standard CESK machine has a store that maps freshly allocated addresses to single values.
%
Typically, memory allocation is specified as $\maddr \notin \dom(\mstore)$, but this is just a specification, not an implementation.
%
Concretely, we rely on an allocation function $\alloc : \CESKstart \to \Addr$ that will produce an address for the machine to use.
%
The $\alloc$ function should be considered a parameter of the machine; we 
obtain different machines by instantiating different allocation strategies.
%
The allocation function need not always produce a fresh address (and finite
allocation strategies generate finite approximations of the running of a program).
%
It is important to observe that if $\alloc$ produces a previously
allocated address, the previous tenant of the address is not
forgotten, just merged as a \emph{possible} result.  This is reflected
in the use of $\sqcup$ when the store is updated, defined as:
\begin{align*}
  \joinm{\mstore}{\maddr}{s} &= \extm{\mstore}{\maddr}{\mstore(\maddr)\cup\set{s}}\text.
\end{align*}
Conversely, when an address is dereferenced, as in the case of
evaluating a variable or popping the top frame of the stack, a single
element is non-deterministically selected among those in the set for
that address.  If $\alloc$ always produces fresh addresses, this set
machinery degenerates so that singleton sets reside at every allocated
address and the non-determinism of address dereference vanishes.

In addition to the explicit machine management of memory, machine
states carry timestamps, written with subscripts.  Timestamps are
drawn from an unspecified set and rely on another parameter of the
machine: the $\tick$ function, which will play a role in the
approximating semantics, but for now can be thought of as a simple
counter.

% We use a slightly non-standard CESK machine that allows $\alloc$ to produce arbitrary addresses instead of just fresh ones.

%
%% The allocation function is a \emph{parameter}; the ``machine'' AAM produces is really a machine \emph{schema}, since we can vary the allocator to produce both and anything in between a concrete and abstract semantics.
%% %
%% That said, we will still refer to the refactored semantics as machines going forward.
%% %
%% %The semantics of this CESK machine is shown in \autoref{fig:cesk-semantics}.
%% %
%% The only differences from the standard presentation are in the use of $\in$ instead of $=$ to make lookups non-deterministic, and weak updates with $\sqcup$ instead of strong updates to allow sound reuse of addresses:
%% %

%
%




\begin{figure}\centering
  \begin{tabular}{rlrl}
    $\mstate \in \CESKstart$ &\hspace{-3mm}$::= \tpl{\mexpr, \menv, \mstore, \mkont}_t$  &
%    $\mlam \in \Lam$ &\hspace{-3mm}$::= \slam{\mvar}{\mexpr}$ & $\mval \in \Value$ &\hspace{-3mm}$::= (\mlam,\menv)$ \\
$\mkont \in \Kont$ &\hspace{-3mm}$::= \epsilon \alt \kcons{\mkframe}{\maddr}$\\
 $\mstore \in \Store$ &\hspace{-3mm}$= \Addr \finto \wp(\Storeable)$ &
%    $\mkframe \in \Frame$ &\multicolumn{2}{l}{\hspace{-3mm}$::= \appl{\mexpr,\menv} \alt \appr{\mval}$} \\
%    $\mkont \in \Kont$ &\hspace{-3mm}$= \Frame^*$ & & \\
%    $\mvar \in \Var$ &\hspace{-3mm} a set &
 $\maddr,\maddralt$ &\hspace{-3mm}$\in \Addr$\\ % \hspace{-3mm} a set \\
    $\menv \in \Env$ &\hspace{-3mm}$= \Var \finto \Addr$ 
& $\mtime,\mtimealt$ &\hspace{-3mm}$\in \Time$\\
    $\alloc$ &\hspace{-3mm}$: \CESKstart \to \Addr$ &
$\Storeable$ &\hspace{-3mm}$::= \mkont \alt \mval$
  \end{tabular}
\\[2mm]

  $\mstate \stepto \mstate'$ \quad $\maddr = \alloc(\mstate)$ \quad $\mtimealt = \tick(\mstate)$ \\
  \begin{tabular}{r|l}
    \hline\vspace{-3mm}\\
    $\tpl{\svar\mvar, \menv, \mstore,\mkont}_\mtime$
    &
    $\tpl{\mval, \mstore,\mkont}_\mtimealt$ if $\mval \in \mstore(\menv(\mvar))$
    \\
% Application
    $\tpl{\sapp{\mexpri0}{\mexpri1},\menv,\mstore,\mkont}_\mtime$
    &
    $\tpl{\mexpri0,\menv,\mstore',\kcons{\appl{\mexpri1,\menv}}{\maddr}}_\mtimealt$ \\
    where & $\mstore' = \joinm{\mstore}{\maddr}{\mkont}$
    \\
% Arg eval
    $\tpl{\mval,\menv, \mstore,\kcons{\appl{\mexpr,\menv'}}{\maddralt}}_\mtime$
    &
    $\tpl{\mexpr,\menv',\mstore,\kcons{\appr{\mval,\menv}}{\maddralt}}_\mtimealt$
    \\
% Function call
    $\tpl{\mval,\mstore,\kcons{\appr{\slam{\mvar}{\mexpr},\menv}}{\maddralt}}_\mtime$
    &
    $\tpl{\mexpr,\menv',\mstore',\mkont}_\mtimealt$ if $\mkont \in \mstore(\maddralt)$ \\
    where & $\menv' = \extm{\menv}{\mvar}{\maddr}$ \\
          & $\mstore' = \joinm{\mstore}{\maddr}{\mval}$
  \end{tabular} \\
  \caption{$\CESKstart$ semantics}
  \label{fig:ceskstart}
\end{figure}

%% \begin{figure}
%%   \centering
%%   $\mstate \stepto \mstate'$ \qquad $\maddr = \alloc(\mstate)$ \\
%%   \begin{tabular}{r|l}
%%     \hline\vspace{-3mm}\\
%%     $\tpl{\svar\mvar, \menv, \mstore, \mkont}$
%%     &
%%     $\tpl{\mval, \mstore, \mkont}$ if $\mval \in \mstore(\menv(\mvar))$
%%     \\
%% % Application
%%     $\tpl{\sapp{\mexpri0}{\mexpri1},\menv,\mstore,\mkont}$
%%     &
%%     $\tpl{\mexpri0,\menv,\mstore,\kcons{\appl{\mexpri1,\menv}}{\mkont}}$
%%     \\
%% % Arg eval
%%     $\tpl{\mval, \mstore,\kcons{\appl{\mexpr,\menv}}{\mkont}}$
%%     &
%%     $\tpl{\mexpr,\menv,\mstore,\kcons{\appr{\mval}}{\mkont}}$
%%     \\
%% % Function call
%%     $\tpl{\mval,\mstore,\kcons{\appr{\slam{\mvar}{\mexpr},\menv}}{\mkont}}$
%%     &
%%     $\tpl{\mexpr,\extm{\menv}{\mvar}{\maddr},\joinm{\mstore}{\maddr}{\mval},\mkont}$
%%   \end{tabular}
%%   \caption{CESK machine}
%%   \label{fig:cesk-semantics}
%% \end{figure}

\paragraph{Taming computability via \(\boldsymbol\alloc\):}
Allocation is key for both precision and abstraction power.
%
Fresh allocation concretely runs a program, precisely predicting everything; of course, analysis is undecidable.
%
If the allocator has a finite codomain, and there are no recursive datatypes in a state's representation, then the state space is finite -- the machine is a finite state machine.
%
If the allocator freshly allocates addresses for the stack representation, but is finite for everything else, the semantics is indistinguishable from just using the recursive representation of the stack.
%
If we have just the stack as an unbounded data structure, then the machine has finitely many stack frames and finitely many other components of the state -- the machine is a pushdown system.


In order to make the iteration of $\betastep$ on a program finite and
therefore have a computable graph, we must finitize the recursive
spaces in which the machine \emph{creates new values}.
%
AAM dictates that finitization can be centralized to one place,
address allocation, by redirecting values in recursive positions
through the store as we've already done.
%
% Expressions, though recursive, do not need this step because they are only destructed.
%
To see why the reachable states of a program are finite, let's
consider each component of a machine state.  First, there is the
expression component.  For a given program, there a finite set of
subexpressions and no machine transition produces new expressions, so
there can only be a finite set of reachable expression components.
The environment component maps variable names to addresses.  Variables
are a subset of expressions, so there are a finite set of variables
(for a given program).  Since we've assumed $\alloc$ produces
addresses from a finite set, there can only be a finite set of
environments (the domain and range are finite sets).  The store
compoent maps addresses (finite) to values or continuations.  Values
are pairs of lambda-terms (a subset of expressions, hence finite) and
environments (as we've just determined: finite as well), so there are
a finite number of values.  Continuations are pairs of frames and
addresses.  Frames consist of expression-environment pairs or values,
both of which are finite sets, so continuations are finite too.
Therefore heaps are finite.  The last component is a continuation,
which we've just argued is finite.  Therefore, starting from an
initial configuration there can only be a finite number of states that
can be reached by the $\betastep$ relation assuming a finite $\alloc$
function.


%The $\Expr$ space is finite for each program, the size of which is the number of subexpressions in the program.
%


%
%The runaway recursion is in $\Kont$: the tail of the continuation cons is recursive.
%
%So then conses of frames in $\Kont$ instead allocate an address, update the heap with the recursive value, and use the address in place of the recursive value.
%
%To help an $\alloc : \State \to \Addr$ function choose its addresses, the state space can be extended with an arbitrary pointed space that can be updated each step.
%
%The original AAM paper calls this pointed space and update function $(\Time,\mtime_0)$ and $\tick$ respectively.
%
%The point, $\mtime_0$, is for the initial state's timestamp.
%
%Later work on widening~\citep{dvanhorn:Hardekopf2014Widening} suggests less arbitrary constructions, and to think of $\Time$ as a space of abstract traces, though the ``abstraction'' need not be sound~\citep{dvanhorn:Might2009Posteriori}.
%
%The timestamp machinery is important to implementing specific allocation schemes, but is primarily noise for the exposition in this paper, so we omit it past the first couple demonstrations.

%% The new semantic spaces in \autoref{fig:ceskstart-spaces} form the $\CESKstart$ machine.
%% %
%% The semantics of this machine follow the weak update and non-deterministic lookup principles of AAM in \autoref{fig:ceskstart-semantics}.

%% \begin{figure}
%%   \centering
%%   \begin{tabular}{rlrl}
%%     $\mastate \in \sa{CESK}_t$ &\hspace{-3mm}$::= \tpl{\mexpr, \menv, \mastore, \makont}_\mtime$ & & \\
%%     $\mastore \in \sa{Store}$ &\hspace{-3mm}$= \Addr \to \wp(\Storeable)$ & $\makont \in \sa{Kont}$ &\hspace{-3mm}$::= \epsilon \alt \kcons{\mkframe}{\maddr}$ \\
%%     \hspace{-1mm}$\mtime,\mtimealt$ &\hspace{-3mm}$\in \Time$ & \hspace{-1mm}$\Storeable$ &\hspace{-3mm}$::= \makont \alt \mval$
%%   \end{tabular}
%%   \caption{$\CESKstart$ semantic spaces}
%%   \label{fig:ceskstart-spaces}
%% \end{figure}


If we run the $\CESKstart$ semantics to explore all possible states, we get a sound approximation of all paths that the $\CESK$ machine will explore, \emph{regardless of the particular $\alloc$ function used}; any $\alloc$ function is sound.
%
This paper will give a more focused view of the $\Kont$ component.
%
We said that when just $\Kont$ is unbounded, we have a pushdown system.
%
Pushdown systems cannot be naively \emph{run} to find all states and describe all paths the $\CESK$ machine can explore; the state space is infinite, therefore this strategy may not terminate.
%
The pushdown limitation is special because we can always recognize non-termination, stop, and describe the entire state space.
%
We show that a simple change in state representation can provide this functionality.
%
We regain the ability to just \emph{run} the semantics and get a finite object that describes all possible paths in the $\CESK$ machine, but with better precision than before.
